{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Sparsity\n",
    "\n",
    "This notebook seeks to investigate the effect of extreme sparsity on the performance of the model. This should be the \"raw\" approach to the problem without any assumptions other than in regards to the model.\n",
    "Because of the extreme sparsity the choice of clustering model is important. The model should be able to handle the extreme sparsity and be able to cluster the data in a way that is meaningful. The model should also be able to handle the large number of features. \n",
    "\n",
    "The considered and tested models are:\n",
    "- K-Means with sparse initialization.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/rtdqdzb153306j602rcl9s740000gn/T/ipykernel_30556/1139079966.py:7: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  from scipy.sparse.csr import csr_matrix\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from typing import Dict\n",
    "from scipy.sparse.csr import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cuisines: 20\n"
     ]
    }
   ],
   "source": [
    "# load data from json\n",
    "with open('../data/raw/train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# number of cuisines\n",
    "cuisines = [d['cuisine'] for d in data]\n",
    "number_of_cuisines = len(set(cuisines))\n",
    "print(f\"Number of cuisines: {number_of_cuisines}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'russian', 'greek', 'french', 'jamaican', 'indian', 'japanese', 'italian', 'chinese', 'filipino', 'moroccan', 'vietnamese', 'spanish', 'mexican', 'brazilian', 'british', 'thai', 'korean', 'irish', 'cajun_creole', 'southern_us'}\n"
     ]
    }
   ],
   "source": [
    "print(set(cuisines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate the corpus of all ingredients.\n",
    "updated_data = []\n",
    "corpus = []\n",
    "for i, entry in enumerate(data):\n",
    "    entry_ingredients = [\n",
    "        ingredient.replace(\" \", \"\") for ingredient in entry[\"ingredients\"]\n",
    "    ]\n",
    "    corpus.append(\" \".join(entry_ingredients))\n",
    "    entry[\"representation_idx\"] = i\n",
    "    updated_data.append(entry.copy())\n",
    "\n",
    "# 2. Create the count vectors.\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "one_hot_representation = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert one_hot_representation.shape[0] == len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, labels,n_runs=5):\n",
    "    name = km.__class__.__name__\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering done in 2.42 ± 0.61 s \n",
      "Homogeneity: 0.174 ± 0.013\n",
      "Completeness: 0.159 ± 0.011\n",
      "V-measure: 0.166 ± 0.012\n",
      "Adjusted Rand-Index: 0.058 ± 0.006\n",
      "Silhouette Coefficient: -0.002 ± 0.002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=number_of_cuisines,\n",
    "    max_iter=100,\n",
    "    n_init=5,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, one_hot_representation, cuisines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering done in 0.50 ± 0.09 s \n",
      "Homogeneity: 0.103 ± 0.031\n",
      "Completeness: 0.094 ± 0.029\n",
      "V-measure: 0.099 ± 0.030\n",
      "Adjusted Rand-Index: 0.035 ± 0.013\n",
      "Silhouette Coefficient: 0.024 ± 0.003\n"
     ]
    }
   ],
   "source": [
    "# Run the same experiment but with regions instead\n",
    "with open(\"../data/regions.json\") as infile:\n",
    "    regions = json.load(infile)\n",
    "\n",
    "region_labels = [regions[d[\"cuisine\"]] for d in data]\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=len(set(region_labels)),\n",
    "    max_iter=100,\n",
    "    n_init=5,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, one_hot_representation, region_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cuisine, find the 10 most common ingredients\n",
    "cuisine_ingredients = defaultdict(list)\n",
    "for entry in updated_data:\n",
    "    cuisine = entry[\"cuisine\"]\n",
    "    ingredients = entry[\"ingredients\"]\n",
    "    cuisine_ingredients[cuisine].extend(ingredients)\n",
    "\n",
    "ingredients_to_use = list()\n",
    "\n",
    "for cuisine, ingredients in cuisine_ingredients.items():\n",
    "    from collections import Counter\n",
    "    c = Counter(ingredients)\n",
    "    most_common = c.most_common(10)\n",
    "    ingredients_to_use.extend([ingredient for ingredient, _ in most_common])\n",
    "\n",
    "# remove duplicates\n",
    "ingredients_to_use = list(set(ingredients_to_use))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate the corpus of all ingredients.\n",
    "updated_data = []\n",
    "corpus = []\n",
    "for i, entry in enumerate(data):\n",
    "    entry_ingredients = [\n",
    "        ingredient.replace(\" \", \"\") for ingredient in entry[\"ingredients\"]\n",
    "    ]\n",
    "    corpus.append(\" \".join(entry_ingredients))\n",
    "    entry[\"representation_idx\"] = i\n",
    "    updated_data.append(entry.copy())\n",
    "\n",
    "# 2. Create the count vectors.\n",
    "vectorizer = CountVectorizer(vocabulary=ingredients_to_use)\n",
    "one_hot_representation = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 63)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering done in 0.19 ± 0.07 s \n",
      "Homogeneity: 0.028 ± 0.004\n",
      "Completeness: 0.045 ± 0.006\n",
      "V-measure: 0.035 ± 0.004\n",
      "Adjusted Rand-Index: 0.022 ± 0.005\n",
      "Silhouette Coefficient: 0.212 ± 0.008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=len(set(region_labels)),\n",
    "    max_iter=100,\n",
    "    n_init=5,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, one_hot_representation, cuisines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering done in 0.20 ± 0.05 s \n",
      "Homogeneity: 0.021 ± 0.004\n",
      "Completeness: 0.019 ± 0.003\n",
      "V-measure: 0.020 ± 0.003\n",
      "Adjusted Rand-Index: 0.013 ± 0.002\n",
      "Silhouette Coefficient: 0.209 ± 0.007\n"
     ]
    }
   ],
   "source": [
    "# Run the same experiment but with regions instead\n",
    "with open(\"../data/regions.json\") as infile:\n",
    "    regions = json.load(infile)\n",
    "\n",
    "region_labels = [regions[d[\"cuisine\"]] for d in data]\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=len(set(region_labels)),\n",
    "    max_iter=100,\n",
    "    n_init=5,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, one_hot_representation, region_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering done in 4.98 ± 0.07 s \n",
      "Homogeneity: 0.152 ± 0.002\n",
      "Completeness: 0.091 ± 0.001\n",
      "V-measure: 0.114 ± 0.001\n",
      "Adjusted Rand-Index: 0.024 ± 0.000\n",
      "Silhouette Coefficient: 0.621 ± 0.011\n"
     ]
    }
   ],
   "source": [
    "# Run the same \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=len(set(cuisines))*10,\n",
    "    max_iter=100,\n",
    "    n_init=5,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, one_hot_representation, cuisines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect top ten ingredients for each region\n",
    "cuisine_ingredients = defaultdict(list)\n",
    "for entry in updated_data:\n",
    "    cuisine = entry[\"cuisine\"]\n",
    "    ingredients = entry[\"ingredients\"]\n",
    "    cuisine_ingredients[regions[cuisine]].extend(ingredients)\n",
    "\n",
    "ingredients_to_use = list()\n",
    "\n",
    "for cuisine, ingredients in cuisine_ingredients.items():\n",
    "    from collections import Counter\n",
    "    c = Counter(ingredients)\n",
    "    most_common = c.most_common(10)\n",
    "    ingredients_to_use.extend([ingredient for ingredient, _ in most_common])\n",
    "\n",
    "# remove duplicates\n",
    "ingredients_to_use = list(set(ingredients_to_use))\n",
    "\n",
    "len(ingredients_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 30)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Generate the corpus of all ingredients.\n",
    "updated_data = []\n",
    "corpus = []\n",
    "for i, entry in enumerate(data):\n",
    "    entry_ingredients = [\n",
    "        ingredient.replace(\" \", \"\") for ingredient in entry[\"ingredients\"]\n",
    "    ]\n",
    "    corpus.append(\" \".join(entry_ingredients))\n",
    "    entry[\"representation_idx\"] = i\n",
    "    updated_data.append(entry.copy())\n",
    "\n",
    "# 2. Create the count vectors.\n",
    "vectorizer = CountVectorizer(vocabulary=ingredients_to_use)\n",
    "one_hot_representation = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.vocabulary_\n",
    "one_hot_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering done in 0.26 ± 0.03 s \n",
      "Homogeneity: 0.036 ± 0.004\n",
      "Completeness: 0.024 ± 0.002\n",
      "V-measure: 0.029 ± 0.003\n",
      "Adjusted Rand-Index: 0.013 ± 0.001\n",
      "Silhouette Coefficient: 0.456 ± 0.016\n"
     ]
    }
   ],
   "source": [
    "# Run the same \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=len(set(region_labels))*2,\n",
    "    max_iter=100,\n",
    "    n_init=5,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, one_hot_representation, region_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31df09309b7e51726da855a4f598f2d34b12b6bac7bb5da1e6506c78f0d7a898"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
